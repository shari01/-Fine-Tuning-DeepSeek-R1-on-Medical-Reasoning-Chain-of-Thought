{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z87270vtsVl7",
        "outputId": "774da019-5ae5-4063-b61b-e432fc767e00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.11/dist-packages (2025.4.1)\n",
            "Collecting git+https://github.com/unslothai/unsloth.git\n",
            "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-req-build-aqo27lif\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-req-build-aqo27lif\n",
            "  Resolved https://github.com/unslothai/unsloth.git to commit 7a8f99e1890213cdd01a3ab6c3e13174a96e8220\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: unsloth\n",
            "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unsloth: filename=unsloth-2025.4.1-py3-none-any.whl size=192657 sha256=b7e4393a33bbf608fcf8eda13c83a175028133614e249f39351e1793e359bb03\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hlakti8e/wheels/d1/17/05/850ab10c33284a4763b0595cd8ea9d01fce6e221cac24b3c01\n",
            "Successfully built unsloth\n",
            "Installing collected packages: unsloth\n",
            "  Attempting uninstall: unsloth\n",
            "    Found existing installation: unsloth 2025.4.1\n",
            "    Uninstalling unsloth-2025.4.1:\n",
            "      Successfully uninstalled unsloth-2025.4.1\n",
            "Successfully installed unsloth-2025.4.1\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Setup Environment and Install Required Libraries\n",
        "# first add api in side bar of colab wiht HF_TOKKEN and WANDB_API_TOKEN\n",
        "# Install the basic Unsloth package from PyPI\n",
        "# Unsloth helps in faster and memory-efficient fine-tuning of large language models (LLMs).\n",
        "!pip install unsloth\n",
        "\n",
        "# Force-reinstall the latest Unsloth version directly from GitHub\n",
        "# - '--force-reinstall' ensures any old version is removed.\n",
        "# - '--no-cache-dir' prevents using any locally cached packages.\n",
        "# - '--no-deps' skips installing dependencies again (faster).\n",
        "# This guarantees you are working with the newest Unsloth features and bug fixes.\n",
        "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6w5Z3IRI5Ciz",
        "outputId": "5eabea9b-242e-44c2-e552-086247d184c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "Unsloth: Failed to patch SmolVLMForConditionalGeneration forward function.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Import Necessary Libraries\n",
        "\n",
        "# Import FastLanguageModel from Unsloth\n",
        "# - Fast optimized wrapper for loading and handling LLMs with 4-bit quantization and flash attention.\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Import torch\n",
        "# - Core deep learning library for handling tensors and model operations.\n",
        "import torch\n",
        "\n",
        "# Import SFTTrainer\n",
        "# - Trainer from TRL (Transformer Reinforcement Learning) for supervised fine-tuning (SFT) tasks.\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Import utility function to check if bfloat16 is supported on current hardware (for mixed precision training).\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# Import Hugging Face Hub login utility\n",
        "# - Needed to authenticate and access models from HuggingFace Hub.\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Import TrainingArguments\n",
        "# - Class to define hyperparameters and settings for training a transformer model.\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Import load_dataset\n",
        "# - Used to load datasets from HuggingFace Datasets library (local or online).\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Import Weights and Biases (wandb)\n",
        "# - For experiment tracking, live visualization of training metrics (optional but useful).\n",
        "import wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnNkaxSa5EG8"
      },
      "outputs": [],
      "source": [
        "# Step 4: Authenticate Hugging Face Account using Token\n",
        "\n",
        "# Import Google Colab's userdata module\n",
        "# - Allows secure access to stored private data like API tokens inside Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the Hugging Face API token stored securely in userdata\n",
        "# - 'HF_TOKEN' must be added manually in your Colab session settings.\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Login to Hugging Face Hub\n",
        "# - Authenticates your session so you can load, fine-tune, and push models to Hugging Face.\n",
        "login(hf_token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbMzRW7Q-rI1",
        "outputId": "60c53059-e7f6-481d-c1ad-9bba0f5688b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU device: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Optional: Check GPU Availability\n",
        "\n",
        "# Import torch\n",
        "# - Core deep learning library, also used to check hardware (GPU) availability.\n",
        "import torch\n",
        "\n",
        "# Print whether CUDA (GPU support) is available\n",
        "# - CUDA is NVIDIA's technology that accelerates deep learning operations.\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "# Print the name of the GPU device if available\n",
        "# - If a GPU is available, it prints the specific GPU model (like Tesla T4, A100).\n",
        "# - If no GPU, it prints \"No GPU\".\n",
        "print(\"GPU device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9uJ0kyU5EJC",
        "outputId": "6c2a0b78-42b7-4f8e-c646-8f5b3529c10a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.4.1: Fast Llama patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Setup Pretrained DeepSeek-R1 Model for Fine-Tuning\n",
        "\n",
        "# Define the model name\n",
        "# - We are using DeepSeek R1 Distilled Llama 8B model from Hugging Face.\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
        "\n",
        "# Set the maximum sequence length\n",
        "# - Defines how many tokens the model can handle per input during fine-tuning or inference.\n",
        "# - 2048 tokens = about 1000–1600 words depending on tokenization.\n",
        "max_sequence_length = 2048\n",
        "\n",
        "# Set datatype\n",
        "# - dtype is kept None here to automatically choose the best format (float16, bfloat16).\n",
        "dtype = None\n",
        "\n",
        "# Choose loading mode\n",
        "# - load_in_4bit=True loads the model in 4-bit precision for huge memory savings.\n",
        "# - Can also choose 8-bit, 16-bit, or 32-bit loading if you have enough GPU memory.\n",
        "load_in_4bit = True\n",
        "\n",
        "# Load the model and tokenizer using Unsloth's FastLanguageModel\n",
        "# - Fast loading with support for 4-bit quantization, flash attention, and memory optimization.\n",
        "# - Needs Hugging Face authentication token to download private or large models.\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_sequence_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    token = hf_token\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywwTJ-EX-wZS"
      },
      "outputs": [],
      "source": [
        "# Step 6: Setup the System Prompt Template\n",
        "\n",
        "# Define the system prompt style\n",
        "# - This prompt sets the instructions for how the model should behave during fine-tuning or inference.\n",
        "# - Helps the model focus on providing structured, step-by-step reasoning in its answers.\n",
        "prompt_style = \"\"\"\n",
        "Below is a task description along with additional context provided in the input section. Your goal is to provide a well-reasoned response that effectively addresses the request.\n",
        "\n",
        "Before crafting your answer, take a moment to carefully analyze the question. Develop a clear, step-by-step thought process to ensure your response is both logical and accurate.\n",
        "\n",
        "### Task:\n",
        "You are a medical expert specializing in clinical reasoning, diagnostics, and treatment planning. Answer the medical question below using your advanced knowledge.\n",
        "\n",
        "### Query:\n",
        "{}\n",
        "\n",
        "### Answer:\n",
        "<think>{}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXlU4R1M-x9T",
        "outputId": "af8e576f-c4a5-4b96-9938-a2d34ba2930a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"<｜begin▁of▁sentence｜>\\nBelow is a task description along with additional context provided in the input section. Your goal is to provide a well-reasoned response that effectively addresses the request.\\n\\nBefore crafting your answer, take a moment to carefully analyze the question. Develop a clear, step-by-step thought process to ensure your response is both logical and accurate.\\n\\n### Task:\\nYou are a medical expert specializing in clinical reasoning, diagnostics, and treatment planning. Answer the medical question below using your advanced knowledge.\\n\\n### Query:\\nA 61-year-old woman with a long history of involuntary urine loss during activities like coughing or\\n              sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings,\\n              what would cystometry most likely reveal about her residual volume and detrusor contractions?\\n\\n### Answer:\\n<think>\\nOkay, so I'm trying to figure out what cystometry would show for this 61-year-old woman. Let me start by breaking down the information given. She has a history of involuntary urine loss when she coughs or sneezes, but she doesn't leak at night. That makes me think about possible causes for her symptoms.\\n\\nFirst, the Q-tip test was done during her gynecological exam. I remember that the Q-tip test is used to check for urethral obstruction. The provider inserts a Q-tip catheter into the urethra and measures the pressure. If the pressure is high, it suggests that the urethral opening is narrow, leading to difficulty in voiding and possibly causing urinary retention or other symptoms.\\n\\nNow, considering her symptoms—involuntary leakage during activities like coughing or sneezing—this is classic for stress urinary incontinence (SUI). SUI typically occurs due to a weak pelvic floor muscle, which can't effectively support the urethra during activities that increase abdominal pressure. However, she doesn't have leakage at night, which rules out nighttime incontinence, more commonly associated with factors like aging, obesity, or chronic conditions affecting the bladder or nervous system.\\n\\nCystometry, also known as a bladder diary or filling test, is used to assess how the bladder behaves during filling and emptying. It can reveal things like residual volume, which is the amount of urine left in the bladder after emptying, and detrusor contractions, which are involuntary muscle contractions of the bladder.\\n\\nIf her Q-tip test results were abnormal, suggesting urethral obstruction, then during cystometry, her bladder might not empty completely, leading to a higher residual volume. Alternatively, if the issue is with the bladder muscle itself, like a weak detrusor, the cystometry might show increased contractions or impaired emptying despite normal filling.\\n\\nWait, but her main symptom is SUI, which is more about the urethral resistance. So in that case, the detrusor contractions might not be the primary issue. Instead, the problem is that the urethral sphincter isn't opening properly, leading to incomplete emptying and leakage upon increased abdominal pressure.\\n\\nBut cystometry typically focuses on the bladder's capacity and how it empties. If her bladder can't empty completely, residual volume would be high. Also, if there's urethral obstruction, the bladder may have difficulty emptying, leading to retention and possible overflow incontinence.\\n\\nSo putting it all together, the cystometry would likely show an increased residual volume because her bladder isn't emptying fully, and possibly some detrusor contractions if there's an issue with the bladder muscle. But given her symptoms and the Q-tip test result, the primary issue is more likely urethral, leading to incomplete emptying and thus higher residual volume.\\n</think>\\n\\nCystometry for this 61-year-old woman would most likely reveal an elevated residual volume in her bladder due to incomplete emptying. This is consistent with findings from the Q-tip test, which suggests urethral obstruction, a common cause of stress urinary incontinence. The primary issue appears to be related to the urethral sphincter's inability to open properly during activities that increase abdominal pressure, leading to incomplete voiding. Therefore, the cystometry would indicate a higher residual volume, reflecting the bladder's inability to fully empty during normal urination attempts.<｜end▁of▁sentence｜>\"]\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Run Inference on the Fine-Tuned Model\n",
        "\n",
        "# Define a test question for the model\n",
        "# - Medical diagnostic question to check if the model can generate clinical reasoning answers.\n",
        "question = \"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or\n",
        "              sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings,\n",
        "              what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\"\"\n",
        "\n",
        "# Set the model in inference (evaluation) mode\n",
        "# - Disables gradient computation, saving memory and speeding up generation.\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Tokenize the input\n",
        "# - Converts text into model-readable token IDs.\n",
        "# - Applies the earlier defined prompt format.\n",
        "# - Moves input tensors to GPU (\"cuda\") for faster processing.\n",
        "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate a response\n",
        "# - Runs the model forward pass to predict the answer.\n",
        "# - `max_new_tokens=1200` means model can generate up to 1200 tokens in the answer.\n",
        "# - `use_cache=True` enables faster decoding by caching key/values from attention layers.\n",
        "outputs = model.generate(\n",
        "    input_ids = inputs.input_ids,\n",
        "    attention_mask = inputs.attention_mask,\n",
        "    max_new_tokens = 1200,\n",
        "    use_cache = True\n",
        ")\n",
        "\n",
        "# Decode the output tokens back into human-readable text\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "\n",
        "# Print the final generated response\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQRw7EOj-x_Y",
        "outputId": "36869015-6bc7-4144-826a-dbc88b3571c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<think>\n",
            "Okay, so I'm trying to figure out what cystometry would show for this 61-year-old woman. Let me start by breaking down the information given. She has a history of involuntary urine loss when she coughs or sneezes, but she doesn't leak at night. That makes me think about possible causes for her symptoms.\n",
            "\n",
            "First, the Q-tip test was done during her gynecological exam. I remember that the Q-tip test is used to check for urethral obstruction. The provider inserts a Q-tip catheter into the urethra and measures the pressure. If the pressure is high, it suggests that the urethral opening is narrow, leading to difficulty in voiding and possibly causing urinary retention or other symptoms.\n",
            "\n",
            "Now, considering her symptoms—involuntary leakage during activities like coughing or sneezing—this is classic for stress urinary incontinence (SUI). SUI typically occurs due to a weak pelvic floor muscle, which can't effectively support the urethra during activities that increase abdominal pressure. However, she doesn't have leakage at night, which rules out nighttime incontinence, more commonly associated with factors like aging, obesity, or chronic conditions affecting the bladder or nervous system.\n",
            "\n",
            "Cystometry, also known as a bladder diary or filling test, is used to assess how the bladder behaves during filling and emptying. It can reveal things like residual volume, which is the amount of urine left in the bladder after emptying, and detrusor contractions, which are involuntary muscle contractions of the bladder.\n",
            "\n",
            "If her Q-tip test results were abnormal, suggesting urethral obstruction, then during cystometry, her bladder might not empty completely, leading to a higher residual volume. Alternatively, if the issue is with the bladder muscle itself, like a weak detrusor, the cystometry might show increased contractions or impaired emptying despite normal filling.\n",
            "\n",
            "Wait, but her main symptom is SUI, which is more about the urethral resistance. So in that case, the detrusor contractions might not be the primary issue. Instead, the problem is that the urethral sphincter isn't opening properly, leading to incomplete emptying and leakage upon increased abdominal pressure.\n",
            "\n",
            "But cystometry typically focuses on the bladder's capacity and how it empties. If her bladder can't empty completely, residual volume would be high. Also, if there's urethral obstruction, the bladder may have difficulty emptying, leading to retention and possible overflow incontinence.\n",
            "\n",
            "So putting it all together, the cystometry would likely show an increased residual volume because her bladder isn't emptying fully, and possibly some detrusor contractions if there's an issue with the bladder muscle. But given her symptoms and the Q-tip test result, the primary issue is more likely urethral, leading to incomplete emptying and thus higher residual volume.\n",
            "</think>\n",
            "\n",
            "Cystometry for this 61-year-old woman would most likely reveal an elevated residual volume in her bladder due to incomplete emptying. This is consistent with findings from the Q-tip test, which suggests urethral obstruction, a common cause of stress urinary incontinence. The primary issue appears to be related to the urethral sphincter's inability to open properly during activities that increase abdominal pressure, leading to incomplete voiding. Therefore, the cystometry would indicate a higher residual volume, reflecting the bladder's inability to fully empty during normal urination attempts.<｜end▁of▁sentence｜>\n"
          ]
        }
      ],
      "source": [
        "# Post-processing: Extract and Print Only the Model's Answer\n",
        "\n",
        "# - After generation, the full output includes the full prompt + the answer.\n",
        "# - We split the output at the special marker \"### Answer:\" to isolate only the generated answer.\n",
        "# - Then print the pure answer for clean display.\n",
        "\n",
        "print(response[0].split(\"### Answer:\")[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1rGOKPF-yB1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c29229f30b3140fbac3265f455389397",
            "2f2787b1198f4c0f9ce86174bae02035",
            "fa91aa1179784f919422a4b05b30e624",
            "3052f995e35f48a3aeddfaa4792455ae",
            "8e2d77d8c3454739a0a6c07a9fd7b641",
            "5573460f0d83468fb05c9ba145a37bf7",
            "ae4b63381db2422289aa0ec1021d002f",
            "cb99dcf6219e4f0aa5e23b12a91e2fa4",
            "b3a74a90dde44b5fa2808910920fbcf7",
            "94b1e46e300543c38620254d2187d20e",
            "e94b61933ebc4a3f92a7b1b80e5d1751"
          ]
        },
        "outputId": "14053cbd-2bd1-4c70-d87d-26523bc27677"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/19704 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c29229f30b3140fbac3265f455389397"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Step 8: Setup Fine-Tuning\n",
        "\n",
        "# Load the Medical Reasoning Dataset\n",
        "# - We are using the 'medical-o1-reasoning-SFT' dataset from Hugging Face.\n",
        "# - \"en\" specifies the English version of the dataset.\n",
        "# - 'train[:500]' means we are only loading the first 500 examples for faster testing/training.\n",
        "# - 'trust_remote_code=True' allows loading custom dataset scripts if the dataset repo uses them.\n",
        "medical_dataset = load_dataset(\n",
        "    \"FreedomIntelligence/medical-o1-reasoning-SFT\",\n",
        "    \"en\",\n",
        "    split=\"train[:500]\",\n",
        "    trust_remote_code=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGxWOJVn-yD7",
        "outputId": "96b01f3b-2703-470a-feeb-77b2e6c70eeb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Question': 'A 33-year-old woman is brought to the emergency department 15 minutes after being stabbed in the chest with a screwdriver. Given her vital signs of pulse 110/min, respirations 22/min, and blood pressure 90/65 mm Hg, along with the presence of a 5-cm deep stab wound at the upper border of the 8th rib in the left midaxillary line, which anatomical structure in her chest is most likely to be injured?',\n",
              " 'Complex_CoT': \"Okay, let's figure out what's going on here. A woman comes in with a stab wound from a screwdriver. It's in her chest, upper border of the 8th rib, left side, kind of around the midaxillary line. First thought, that's pretty close to where the lung sits, right?\\n\\nLet's talk about location first. This spot is along the left side of her body. Above the 8th rib, like that, is where a lot of important stuff lives, like the bottom part of the left lung, possibly the diaphragm too, especially considering how deep the screwdriver went.\\n\\nThe wound is 5 cm deep. That sounds pretty deep. I mean, it could definitely reach down to the lung tissue or maybe the diaphragm. Given that it's midaxillary, we're in the territory where the lower lobe of the left lung hangs out. It's also possible there's some intersection with where the diaphragm begins, but the lung feels more probable somehow.\\n\\nNow, her vitals are concerning: elevated heart rate and low blood pressure. This is serious. My gut says that this kind of vital sign picture could mean something like pneumothorax or maybe hemothorax. Both can happen if the lung gets punctured, and they can make the blood pressure drop and the heart rate skyrocket since she's obviously distressed.\\n\\nSo, putting it all together, the most obvious culprit is the lower lobe of the left lung. The wound's depth and her condition point that way. And, yeah, this adds up with pneumothorax or maybe blood collecting in the chest—the kind of stuff that can really mess with breathing and circulation.\\n\\nAlright, with this in mind, it sure seems like the lung is the most likely thing that got hurt here. Makes sense given the situation, where the wound is, considering her symptoms, and the whole setup.\",\n",
              " 'Response': 'In this scenario, the most likely anatomical structure to be injured is the lower lobe of the left lung. The location of the stab wound—at the upper border of the 8th rib in the left midaxillary line—indicates proximity to the lower lobe of the lung. The depth of the wound (5 cm) suggests it is sufficient to reach lung tissue. Her vital signs of elevated heart rate and low blood pressure could signal complications like a pneumothorax or hemothorax, common consequences of lung trauma that would result from a penetrating injury in this area. Given these considerations, the lower lobe of the left lung is the most probable structure injured.'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# View a Single Example from the Dataset\n",
        "\n",
        "# - Access and display the second item (index 1) from the loaded medical dataset.\n",
        "# - Useful to inspect the format and fields of the dataset (e.g., input text, target answer).\n",
        "# - Important step before fine-tuning to make sure the dataset structure matches your model input format.\n",
        "\n",
        "medical_dataset[1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CRtCjhOZ-yGS",
        "outputId": "39e734f1-ca42-48e6-9a82-1acccfc56905"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<｜end▁of▁sentence｜>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Define End-of-Sequence (EOS) Token\n",
        "\n",
        "# - The EOS token (End Of Sequence) is a special token that tells the model\n",
        "#   where the input or output should stop during generation.\n",
        "# - It is important during training to mark the end of each generated answer properly.\n",
        "# - The EOS token is usually something like '</s>' depending on the tokenizer used.\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# Display the EOS token to confirm what symbol it uses\n",
        "EOS_TOKEN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHVFpt51862S"
      },
      "outputs": [],
      "source": [
        "# Step: Define the Training Prompt Style (Updated for Fine-Tuning)\n",
        "\n",
        "# - Define a new training prompt format specifically for fine-tuning.\n",
        "# - This updated prompt now includes the special </think> tag.\n",
        "# - </think> tag clearly marks the end of the \"thought\" or reasoning section before generating the final answer.\n",
        "# - Using structured prompts like this improves model logical flow during training.\n",
        "\n",
        "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\n",
        "Please answer the following medical question.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "<think>\n",
        "{}\n",
        "</think>\n",
        "{}\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrKFL1ec87CP"
      },
      "outputs": [],
      "source": [
        "# Step: Prepare the Data for Fine-Tuning\n",
        "\n",
        "# Define a function to preprocess input data from the dataset\n",
        "def preprocess_input_data(examples):\n",
        "    # Extract fields from the dataset\n",
        "    # - \"Question\" = the medical question asked.\n",
        "    # - \"Complex_CoT\" = chain of thought (step-by-step reasoning).\n",
        "    # - \"Response\" = final answer text.\n",
        "    inputs = examples[\"Question\"]\n",
        "    cots = examples[\"Complex_CoT\"]\n",
        "    outputs = examples[\"Response\"]\n",
        "\n",
        "    texts = []  # Initialize list to hold formatted training examples\n",
        "\n",
        "    # Loop through each sample and format it using the defined prompt style\n",
        "    for input, cot, output in zip(inputs, cots, outputs):\n",
        "        # Fill the training prompt with input question, chain of thought, and response\n",
        "        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "\n",
        "    # Return dictionary format expected by the trainer (a \"texts\" field)\n",
        "    return {\n",
        "        \"texts\": texts,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ae3da035534441788eb2d498d72307b7",
            "057f246b9738407cb8c4726585947ff3",
            "5f2309506079409a985829c3956614a2",
            "7b0fd9a78a424fd5a32f190a881ccfe0",
            "bb7068feca744a22bc76f7513127649b",
            "5303edc2dd5d4518a7ff790e13ca05f5",
            "3b17657ab1024b518fb967b75de2fa40",
            "95d03d84864f4d5f97ea335849828b31",
            "8073a484a58745d7a6eb80336158023a",
            "48651d50cebf4f87bd7f3b24c500ff64",
            "06dafe961a354d828b0db670681f1008"
          ]
        },
        "id": "OTsrjryT_RLF",
        "outputId": "4d2a2600-8077-44b5-f355-54fdbaeb4ada"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae3da035534441788eb2d498d72307b7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Step: Apply Preprocessing to the Dataset\n",
        "\n",
        "# Map the preprocessing function over the entire medical dataset\n",
        "# - 'map' applies 'preprocess_input_data' to each batch of examples.\n",
        "# - 'batched=True' means it processes multiple samples at once (more efficient).\n",
        "# - This will add a new \"texts\" field to each example, formatted for model input.\n",
        "\n",
        "finetune_dataset = medical_dataset.map(preprocess_input_data, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "crLgEm6h9BNB",
        "outputId": "d9c648e0-1726-4019-9efc-1248878f0a72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Below is an instruction that describes a task, paired with an input that provides further context.\\nWrite a response that appropriately completes the request.\\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\\nPlease answer the following medical question.\\n\\n### Question:\\nGiven the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings?\\n\\n### Response:\\n<think>\\nOkay, let's see what's going on here. We've got sudden weakness in the person's left arm and leg - and that screams something neuro-related, maybe a stroke?\\n\\nBut wait, there's more. The right lower leg is swollen and tender, which is like waving a big flag for deep vein thrombosis, especially after a long flight or sitting around a lot.\\n\\nSo, now I'm thinking, how could a clot in the leg end up causing issues like weakness or stroke symptoms?\\n\\nOh, right! There's this thing called a paradoxical embolism. It can happen if there's some kind of short circuit in the heart - like a hole that shouldn't be there.\\n\\nLet's put this together: if a blood clot from the leg somehow travels to the left side of the heart, it could shoot off to the brain and cause that sudden weakness by blocking blood flow there.\\n\\nHmm, but how would the clot get from the right side of the heart to the left without going through the lungs and getting filtered out?\\n\\nHere's where our cardiac anomaly comes in: a patent foramen ovale or PFO. That's like a sneaky little shortcut in the heart between the right and left atria.\\n\\nAnd it's actually pretty common, found in about a quarter of adults, which definitely makes it the top suspect here.\\n\\nSo with all these pieces - long travel, leg clot, sudden weakness - a PFO fits the bill perfectly, letting a clot cross over and cause all this.\\n\\nEverything fits together pretty neatly, so I'd bet PFO is the heart issue waiting to be discovered. Yeah, that really clicks into place!\\n</think>\\nThe specific cardiac abnormality most likely to be found in this scenario is a patent foramen ovale (PFO). This condition could allow a blood clot from the venous system, such as one from a deep vein thrombosis in the leg, to bypass the lungs and pass directly into the arterial circulation. This can occur when the clot moves from the right atrium to the left atrium through the PFO. Once in the arterial system, the clot can travel to the brain, potentially causing an embolic stroke, which would explain the sudden weakness in the left arm and leg. The connection between the recent travel, which increases the risk of deep vein thrombosis, and the neurological symptoms suggests the presence of a PFO facilitating a paradoxical embolism.<｜end▁of▁sentence｜>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Step: View a Preprocessed Example\n",
        "\n",
        "# Access and display the first preprocessed example from the fine-tuning dataset\n",
        "# - This shows how the 'Question', 'Chain of Thought', and 'Response' were combined.\n",
        "# - Useful for verifying that the input text is correctly formatted before training starts.\n",
        "\n",
        "finetune_dataset[\"texts\"][0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyLZqq8R5ELJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "281ad669-f147-4f75-b063-2e76d378cad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.4.1 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "# Step 9: Setup and Apply LoRA Fine-Tuning to the Model\n",
        "\n",
        "# Prepare the model for LoRA fine-tuning using Unsloth's FastLanguageModel wrapper\n",
        "model_lora = FastLanguageModel.get_peft_model(\n",
        "    model = model,  # Base model loaded earlier\n",
        "    r = 16,  # LoRA rank: size of the small adapter matrices inserted into the model layers\n",
        "\n",
        "    # Specify the target modules where LoRA adapters should be injected\n",
        "    # - Attention projections (q_proj, k_proj, v_proj, o_proj)\n",
        "    # - Feedforward network projections (gate_proj, up_proj, down_proj)\n",
        "    target_modules = [\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\"\n",
        "    ],\n",
        "\n",
        "    lora_alpha = 16,  # LoRA scaling factor: controls strength of LoRA updates\n",
        "    lora_dropout = 0,  # Dropout applied inside LoRA (set to 0 for deterministic fine-tuning)\n",
        "    bias = \"none\",  # No bias added in LoRA adapters\n",
        "    use_gradient_checkpointing = \"unsloth\",  # Enable gradient checkpointing to save memory during backpropagation\n",
        "    random_state = 3047,  # Set random seed for reproducibility\n",
        "    use_rslora = False,  # Do not use rank-stable LoRA (advanced, optional technique)\n",
        "    loftq_config = None  # No LoFTQ quantization applied (can be used if combining LoRA + quantization tricks)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_1xufC2Yt3K"
      },
      "outputs": [],
      "source": [
        "# Important: Clean Up Model Before Creating Trainer\n",
        "\n",
        "# - Check if the model has an attribute called '_unwrapped_old_generate'.\n",
        "# - This attribute is sometimes added by Unsloth or during fast generation hacks.\n",
        "# - It can interfere with HuggingFace's Trainer, causing errors when generating outputs.\n",
        "# - If it exists, delete it safely before initializing the trainer.\n",
        "\n",
        "if hasattr(model, '_unwrapped_old_generate'):\n",
        "    del model._unwrapped_old_generate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c305a00c140f434db8ad8ad14955316c",
            "ef1fca4dadbb4a678c362e0b49406f4b",
            "81b450ab70784e1c827e2cbfa825f66a",
            "7b541fff965346b08fd2617ebbac7a6a",
            "81353d14f3d94275bcc4845ef544105a",
            "b8851e8c3b7f46b19d29faada25a370c",
            "b3a98271f84a4885bbd46a0c5574dd73",
            "c4274a9a84214a61bbf71dee2c7a869f",
            "7556358c93844a11a236a07203c1f77c",
            "d1092af317004edcb7d6c4b9fc451a35",
            "b0e2cee1e4af4a18ac0e3c9917779a69"
          ]
        },
        "id": "NenTCAT45ENx",
        "outputId": "82ed8b6d-6c19-4cdd-aec6-d88a651855c5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"texts\"]:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c305a00c140f434db8ad8ad14955316c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Step: Setup the Trainer for Fine-Tuning\n",
        "\n",
        "# Initialize the SFTTrainer (Supervised Fine-Tuning Trainer)\n",
        "trainer = SFTTrainer(\n",
        "    model = model_lora,            # The model with LoRA adapters applied\n",
        "    tokenizer = tokenizer,         # Tokenizer for converting text to tokens\n",
        "    train_dataset = finetune_dataset,  # Preprocessed fine-tuning dataset\n",
        "    dataset_text_field = \"texts\",   # The dataset field name containing formatted input texts\n",
        "    max_seq_length = max_sequence_length,  # Maximum sequence length during training\n",
        "    dataset_num_proc = 1,           # Number of CPU processes for data loading (1 = no multiprocessing)\n",
        "\n",
        "    # Define detailed training arguments\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,  # Train 2 examples per device at a time\n",
        "        gradient_accumulation_steps = 4,  # Accumulate gradients over 4 steps to simulate a batch size of 8\n",
        "        num_train_epochs = 1,             # Train for 1 full pass over the dataset\n",
        "        warmup_steps = 5,                 # Number of warmup steps for learning rate scheduler\n",
        "        max_steps = 60,                   # Stop training after 60 optimization steps\n",
        "        learning_rate = 2e-4,              # Initial learning rate (0.0002)\n",
        "        fp16 = not is_bfloat16_supported(),  # Use 16-bit floats (fp16) if bf16 is not available\n",
        "        bf16 = is_bfloat16_supported(),      # Use bfloat16 precision if available (better stability)\n",
        "        logging_steps = 10,               # Log metrics every 10 steps\n",
        "        optim = \"adamw_8bit\",              # Use 8-bit AdamW optimizer (memory efficient)\n",
        "        weight_decay = 0.01,               # Apply weight decay regularization to prevent overfitting\n",
        "        lr_scheduler_type = \"linear\",      # Linearly decrease learning rate over time\n",
        "        seed = 3407,                       # Set random seed for reproducibility\n",
        "        output_dir = \"outputs\",            # Save model checkpoints and logs into the \"outputs\" directory\n",
        "    ),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "ST9J4i8PZjYy",
        "outputId": "e49531fc-91dd-4c49-9624-86b902c1e422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msheryar-malik\u001b[0m (\u001b[33msheryar-malik-ayass-bioscience\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250427_062753-rpvqzq52</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sheryar-malik-ayass-bioscience/Fine-tune-DeepSeek-R1-on-Medical-CoT-Dataset/runs/rpvqzq52?apiKey=6223296c17c2c58c791d52b3f1229cb8228f907c' target=\"_blank\">unique-eon-1</a></strong> to <a href='https://wandb.ai/sheryar-malik-ayass-bioscience/Fine-tune-DeepSeek-R1-on-Medical-CoT-Dataset?apiKey=6223296c17c2c58c791d52b3f1229cb8228f907c' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/sheryar-malik-ayass-bioscience/Fine-tune-DeepSeek-R1-on-Medical-CoT-Dataset?apiKey=6223296c17c2c58c791d52b3f1229cb8228f907c' target=\"_blank\">https://wandb.ai/sheryar-malik-ayass-bioscience/Fine-tune-DeepSeek-R1-on-Medical-CoT-Dataset?apiKey=6223296c17c2c58c791d52b3f1229cb8228f907c</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/sheryar-malik-ayass-bioscience/Fine-tune-DeepSeek-R1-on-Medical-CoT-Dataset/runs/rpvqzq52?apiKey=6223296c17c2c58c791d52b3f1229cb8228f907c' target=\"_blank\">https://wandb.ai/sheryar-malik-ayass-bioscience/Fine-tune-DeepSeek-R1-on-Medical-CoT-Dataset/runs/rpvqzq52?apiKey=6223296c17c2c58c791d52b3f1229cb8228f907c</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Do NOT share these links with anyone. They can be used to claim your runs."
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Step: Setup Weights and Biases (WandB) for Experiment Tracking\n",
        "\n",
        "# Import Colab userdata to securely fetch private tokens\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the WANDB API token from Colab's secure storage\n",
        "# - 'WANDB_API_TOKEN' must be set in the Colab environment for this to work.\n",
        "wnb_token = userdata.get(\"WANDB_API_TOKEN\")\n",
        "\n",
        "# Login to Weights and Biases using the retrieved token\n",
        "# - Enables tracking training metrics, model checkpoints, and logs automatically.\n",
        "wandb.login(key=wnb_token)\n",
        "\n",
        "# Initialize a new WandB run\n",
        "run = wandb.init(\n",
        "    project='Fine-tune-DeepSeek-R1-on-Medical-CoT-Dataset',  # Project name in your WandB account\n",
        "    job_type=\"training\",  # Label this run as a \"training\" job\n",
        "    anonymous=\"allow\"     # Allow anonymous access if the token is not linked to a public account\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "irR021n75EP4",
        "outputId": "715c48a3-6c30-4f0c-864f-803fb89eff77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 500 | Num Epochs = 1 | Total steps = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 17:16, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.931200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.424500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.406700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.365000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.391400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.369400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Step: Start the Fine-Tuning Process\n",
        "\n",
        "# Begin training the model using the SFTTrainer\n",
        "# - This will start the supervised fine-tuning process based on the defined dataset, model, and training arguments.\n",
        "# - All training metrics will automatically be logged to WandB if it was initialized.\n",
        "\n",
        "trainer_stats = trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "XwTqmIfz_uNd",
        "outputId": "daa464df-e41c-43ea-c718-e3d91ddecdae"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▂▄▅▇██</td></tr><tr><td>train/global_step</td><td>▁▂▄▅▇██</td></tr><tr><td>train/grad_norm</td><td>█▂▁▁▂▁</td></tr><tr><td>train/learning_rate</td><td>█▇▅▄▂▁</td></tr><tr><td>train/loss</td><td>█▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1.673729227431936e+16</td></tr><tr><td>train/epoch</td><td>0.96</td></tr><tr><td>train/global_step</td><td>60</td></tr><tr><td>train/grad_norm</td><td>0.25691</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.3694</td></tr><tr><td>train_loss</td><td>1.48138</td></tr><tr><td>train_runtime</td><td>1064.0712</td></tr><tr><td>train_samples_per_second</td><td>0.451</td></tr><tr><td>train_steps_per_second</td><td>0.056</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">unique-eon-1</strong> at: <a href='https://wandb.ai/sheryar-malik-ayass-bioscience/Fine-tune-DeepSeek-R1-on-Medical-CoT-Dataset/runs/rpvqzq52?apiKey=6223296c17c2c58c791d52b3f1229cb8228f907c' target=\"_blank\">https://wandb.ai/sheryar-malik-ayass-bioscience/Fine-tune-DeepSeek-R1-on-Medical-CoT-Dataset/runs/rpvqzq52?apiKey=6223296c17c2c58c791d52b3f1229cb8228f907c</a><br> View project at: <a href='https://wandb.ai/sheryar-malik-ayass-bioscience/Fine-tune-DeepSeek-R1-on-Medical-CoT-Dataset?apiKey=6223296c17c2c58c791d52b3f1229cb8228f907c' target=\"_blank\">https://wandb.ai/sheryar-malik-ayass-bioscience/Fine-tune-DeepSeek-R1-on-Medical-CoT-Dataset?apiKey=6223296c17c2c58c791d52b3f1229cb8228f907c</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250427_062753-rpvqzq52/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Step: Finish and Close the WandB Run\n",
        "\n",
        "# - Gracefully close the Weights and Biases (WandB) logging session.\n",
        "# - This finalizes uploading all metrics, logs, artifacts to the WandB dashboard.\n",
        "# - Prevents memory leaks and ensures the run is properly recorded.\n",
        "\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ofzlf5UlfP3o",
        "outputId": "8214fb14-9ef5-4310-be45-1a5e389e0424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"<｜begin▁of▁sentence｜>\\nBelow is a task description along with additional context provided in the input section. Your goal is to provide a well-reasoned response that effectively addresses the request.\\n\\nBefore crafting your answer, take a moment to carefully analyze the question. Develop a clear, step-by-step thought process to ensure your response is both logical and accurate.\\n\\n### Task:\\nYou are a medical expert specializing in clinical reasoning, diagnostics, and treatment planning. Answer the medical question below using your advanced knowledge.\\n\\n### Query:\\nA 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing\\n              but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings,\\n              what would cystometry most likely reveal about her residual volume and detrusor contractions?\\n\\n### Answer:\\n<think>\\nAlright, let's think about this. This woman is 61 and has been dealing with involuntary urine loss for a long time, especially when she coughs or sneezes. That's a classic sign of urinary incontinence, probably due to an overactive bladder. \\n\\nNow, she's done a Q-tip test, which is often used to check for urethral obstruction. If the Q-tip test is negative, it usually means there's no obstruction there. That's good news because if there were a blockage, we might need to do something about it.\\n\\nGiven that she's not leaking at night, it suggests that her bladder capacity isn't being exceeded during sleep. This is a good sign because it means her bladder can hold up to 500 ml without leaking. So, if we look at her bladder, it should be able to hold that volume comfortably.\\n\\nNow, let's think about what cystometry might show. In cystometry, we measure how much her bladder can hold and how it reacts when we add pressure or stimulation. If her bladder is functioning well and she doesn't have any obstructions, we might see that her bladder can hold up to 500 ml without any leakage. That makes sense because she doesn't leak at night, and her bladder seems to be working fine.\\n\\nBut there's also something about detrusor contractions. These contractions happen when her bladder is full and it needs to push out the urine. If her bladder is in good shape, these contractions should be normal. They should help empty her bladder without any issues, which would be ideal for someone who doesn't leak involuntarily.\\n\\nPutting it all together, if her bladder is functioning well, we'd expect to see that it can hold up to 500 ml without any leakage and that the detrusor contractions are normal. That seems to fit with what we know about her symptoms and the Q-tip test results.\\n</think>\\nBased on the information provided, cystometry would most likely reveal that the woman's bladder has a residual volume of up to 500 mL and that her detrusor contractions are normal. This is consistent with her history of not leaking at night, suggesting that her bladder capacity does not exceed 500 mL, and her symptoms align with an overactive bladder without any urethral obstruction, as indicated by a negative Q-tip test.<｜end▁of▁sentence｜>\"]\n"
          ]
        }
      ],
      "source": [
        "# Step 10: Testing the Model After Fine-Tuning\n",
        "\n",
        "# Define a sample medical question to test the fine-tuned model\n",
        "question = \"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing\n",
        "              but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings,\n",
        "              what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\"\"\n",
        "\n",
        "# Set the model in inference (evaluation) mode\n",
        "# - Disables gradient updates and enables efficient generation settings.\n",
        "FastLanguageModel.for_inference(model_lora)\n",
        "\n",
        "# Tokenize the test input\n",
        "# - Format the question using the training prompt style.\n",
        "# - Convert the formatted text into token IDs.\n",
        "# - Move input tensors to GPU (\"cuda\") for faster inference.\n",
        "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate a response from the model\n",
        "# - 'max_new_tokens=1200' limits the maximum number of tokens the model can generate.\n",
        "# - 'use_cache=True' enables faster decoding by caching key/values during generation.\n",
        "outputs = model_lora.generate(\n",
        "    input_ids = inputs.input_ids,\n",
        "    attention_mask = inputs.attention_mask,\n",
        "    max_new_tokens = 1200,\n",
        "    use_cache = True\n",
        ")\n",
        "\n",
        "# Decode the generated tokens back into human-readable text\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "\n",
        "# Print the final model-generated answer\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jhG3-7Lf-ej",
        "outputId": "6971b56e-5fc4-4719-94e3-b3efdef13fe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<think>\n",
            "Alright, let's think about this. This woman is 61 and has been dealing with involuntary urine loss for a long time, especially when she coughs or sneezes. That's a classic sign of urinary incontinence, probably due to an overactive bladder. \n",
            "\n",
            "Now, she's done a Q-tip test, which is often used to check for urethral obstruction. If the Q-tip test is negative, it usually means there's no obstruction there. That's good news because if there were a blockage, we might need to do something about it.\n",
            "\n",
            "Given that she's not leaking at night, it suggests that her bladder capacity isn't being exceeded during sleep. This is a good sign because it means her bladder can hold up to 500 ml without leaking. So, if we look at her bladder, it should be able to hold that volume comfortably.\n",
            "\n",
            "Now, let's think about what cystometry might show. In cystometry, we measure how much her bladder can hold and how it reacts when we add pressure or stimulation. If her bladder is functioning well and she doesn't have any obstructions, we might see that her bladder can hold up to 500 ml without any leakage. That makes sense because she doesn't leak at night, and her bladder seems to be working fine.\n",
            "\n",
            "But there's also something about detrusor contractions. These contractions happen when her bladder is full and it needs to push out the urine. If her bladder is in good shape, these contractions should be normal. They should help empty her bladder without any issues, which would be ideal for someone who doesn't leak involuntarily.\n",
            "\n",
            "Putting it all together, if her bladder is functioning well, we'd expect to see that it can hold up to 500 ml without any leakage and that the detrusor contractions are normal. That seems to fit with what we know about her symptoms and the Q-tip test results.\n",
            "</think>\n",
            "Based on the information provided, cystometry would most likely reveal that the woman's bladder has a residual volume of up to 500 mL and that her detrusor contractions are normal. This is consistent with her history of not leaking at night, suggesting that her bladder capacity does not exceed 500 mL, and her symptoms align with an overactive bladder without any urethral obstruction, as indicated by a negative Q-tip test.<｜end▁of▁sentence｜>\n"
          ]
        }
      ],
      "source": [
        "print(response[0].split(\"### Answer:\")[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDlOpre6gTln",
        "outputId": "4a7a44b8-c5b1-4da7-d81d-19a50e137859"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<think>\n",
            "Okay, so I'm trying to understand this PI3K-AKT pathway and its role in cancer. Let's start by breaking down what PI3K and AKT are. PI3K stands for phosphatidylinositol 3-kinase. It sounds a bit complicated, but I remember that it's part of this signaling pathway that's really important for cell growth and survival. When PI3K is active, it basically takes a lipid called phosphatidylinositol and turns it into phosphatidylinositol 3-phosphate. This is like a signal that tells the cell to grow and survive.\n",
            "\n",
            "Now, what does AKT do? AKT, which is also known as protein kinase B, is a key player in this pathway. It takes the signal from PI3K and modifies it in a way that tells the cell to live longer. AKT does this by phosphorylating other proteins, which in turn affects their activity. For example, it can turn on certain survival signals that keep the cell growing and prevent it from undergoing apoptosis, which is programmed cell death.\n",
            "\n",
            "I've heard that mutations in the PI3K-AKT pathway can lead to cancer. It makes sense because if the pathway is always on, cells might grow uncontrollably. I wonder what kind of mutations could be problematic. Maybe there's a loss of function mutation in PI3K or AKT. If PI3K isn't working, the signaling pathway might not send the right signals, leading to cell death or reduced growth. On the other hand, if AKT is too active, it might cause cells to proliferate too much, leading to cancer development.\n",
            "\n",
            "I'm also curious about the downstream effects of AKT. Does it affect specific pathways in the cell? I think it can influence the mTOR pathway, which is involved in cell growth and metabolism. If the PI3K-AKT pathway is dysregulated, it might lead to changes in these downstream pathways, which could be linked to cancer.\n",
            "\n",
            "What about the role of this pathway in specific cancers? I've heard that cancers like breast, prostate, and liver cancer are often associated with mutations in PI3K or AKT. It seems like there's a pattern where the PI3K-AKT pathway is hyperactive in these cancers, promoting uncontrolled cell growth.\n",
            "\n",
            "In summary, the PI3K-AKT pathway is crucial for cell survival and growth. Mutations in this pathway can lead to cancer development by either turning it on too much or turning it off. This explains why targeting this pathway might be a promising approach for treating cancer.\n",
            "</think>\n",
            "The PI3K-AKT pathway is a critical signaling pathway involved in cell growth, survival, and proliferation. It starts with PI3K (Phosphatidylinositol 3-kinase), which converts phosphatidylinositol into phosphatidylinositol 3-phosphate, acting as a signal for cell survival and growth. AKT (Protein Kinase B) is a key downstream effector in this pathway, playing a significant role in signaling by phosphorylating various targets, which can promote cell survival and inhibit apoptosis.\n",
            "\n",
            "Mutations in this pathway can lead to cancer development. These mutations may include both loss-of-function mutations in PI3K or AKT, which can disrupt the signaling, leading to cell death or reduced growth, and gain-of-function mutations, which can cause excessive cell proliferation. The pathway is particularly associated with cancers such as breast, prostate, and liver cancer, where its hyperactivity may contribute to uncontrolled growth.\n",
            "\n",
            "This pathway's role in cancer highlights the potential for therapeutic intervention, as targeting the PI3K-AKT pathway might offer a promising approach for treating these cancers.<｜end▁of▁sentence｜>\n"
          ]
        }
      ],
      "source": [
        "# Step: Test the Fine-Tuned Model with New Medical Questions\n",
        "\n",
        "# Define a new medical question (or multiple related questions)\n",
        "# - These questions ask about important biological pathways related to cancer.\n",
        "question = \"\"\"Describe the PI3K-AKT pathway and its role in cancer.\n",
        "What genes are involved in the MAPK signaling pathway?\"\"\"\n",
        "\n",
        "# Set the model in inference (evaluation) mode\n",
        "# - Ensures the model runs efficiently without tracking gradients.\n",
        "FastLanguageModel.for_inference(model_lora)\n",
        "\n",
        "# Tokenize the input\n",
        "# - Format the new question using the training prompt style.\n",
        "# - Convert the formatted text into token IDs.\n",
        "# - Move the input tensors to the GPU (\"cuda\") for fast inference.\n",
        "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate a response from the model\n",
        "# - 'max_new_tokens=1200' allows up to 1200 new tokens for detailed answers.\n",
        "# - 'use_cache=True' improves decoding speed.\n",
        "outputs = model_lora.generate(\n",
        "    input_ids = inputs.input_ids,\n",
        "    attention_mask = inputs.attention_mask,\n",
        "    max_new_tokens = 1200,\n",
        "    use_cache = True\n",
        ")\n",
        "\n",
        "# Decode the generated output tokens back into readable text\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "\n",
        "# Post-process the output\n",
        "# - Split the generated response at \"### Answer:\" to isolate only the final clean answer.\n",
        "print(response[0].split(\"### Answer:\")[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8cpFyM2gTRE"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"final_model\")\n",
        "tokenizer.save_pretrained(\"final_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8eKO6AXfP8x"
      },
      "outputs": [],
      "source": [
        "!zip -r final_model.zip final_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVnWnMB1fP_C"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('final_model.zip')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qqDPOZvfQBN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c29229f30b3140fbac3265f455389397": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f2787b1198f4c0f9ce86174bae02035",
              "IPY_MODEL_fa91aa1179784f919422a4b05b30e624",
              "IPY_MODEL_3052f995e35f48a3aeddfaa4792455ae"
            ],
            "layout": "IPY_MODEL_8e2d77d8c3454739a0a6c07a9fd7b641"
          }
        },
        "2f2787b1198f4c0f9ce86174bae02035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5573460f0d83468fb05c9ba145a37bf7",
            "placeholder": "​",
            "style": "IPY_MODEL_ae4b63381db2422289aa0ec1021d002f",
            "value": "Generating train split: 100%"
          }
        },
        "fa91aa1179784f919422a4b05b30e624": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb99dcf6219e4f0aa5e23b12a91e2fa4",
            "max": 19704,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3a74a90dde44b5fa2808910920fbcf7",
            "value": 19704
          }
        },
        "3052f995e35f48a3aeddfaa4792455ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94b1e46e300543c38620254d2187d20e",
            "placeholder": "​",
            "style": "IPY_MODEL_e94b61933ebc4a3f92a7b1b80e5d1751",
            "value": " 19704/19704 [00:01&lt;00:00, 17228.21 examples/s]"
          }
        },
        "8e2d77d8c3454739a0a6c07a9fd7b641": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5573460f0d83468fb05c9ba145a37bf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae4b63381db2422289aa0ec1021d002f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb99dcf6219e4f0aa5e23b12a91e2fa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3a74a90dde44b5fa2808910920fbcf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "94b1e46e300543c38620254d2187d20e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e94b61933ebc4a3f92a7b1b80e5d1751": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae3da035534441788eb2d498d72307b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_057f246b9738407cb8c4726585947ff3",
              "IPY_MODEL_5f2309506079409a985829c3956614a2",
              "IPY_MODEL_7b0fd9a78a424fd5a32f190a881ccfe0"
            ],
            "layout": "IPY_MODEL_bb7068feca744a22bc76f7513127649b"
          }
        },
        "057f246b9738407cb8c4726585947ff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5303edc2dd5d4518a7ff790e13ca05f5",
            "placeholder": "​",
            "style": "IPY_MODEL_3b17657ab1024b518fb967b75de2fa40",
            "value": "Map: 100%"
          }
        },
        "5f2309506079409a985829c3956614a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95d03d84864f4d5f97ea335849828b31",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8073a484a58745d7a6eb80336158023a",
            "value": 500
          }
        },
        "7b0fd9a78a424fd5a32f190a881ccfe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48651d50cebf4f87bd7f3b24c500ff64",
            "placeholder": "​",
            "style": "IPY_MODEL_06dafe961a354d828b0db670681f1008",
            "value": " 500/500 [00:00&lt;00:00, 13683.00 examples/s]"
          }
        },
        "bb7068feca744a22bc76f7513127649b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5303edc2dd5d4518a7ff790e13ca05f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b17657ab1024b518fb967b75de2fa40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95d03d84864f4d5f97ea335849828b31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8073a484a58745d7a6eb80336158023a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48651d50cebf4f87bd7f3b24c500ff64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06dafe961a354d828b0db670681f1008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c305a00c140f434db8ad8ad14955316c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ef1fca4dadbb4a678c362e0b49406f4b",
              "IPY_MODEL_81b450ab70784e1c827e2cbfa825f66a",
              "IPY_MODEL_7b541fff965346b08fd2617ebbac7a6a"
            ],
            "layout": "IPY_MODEL_81353d14f3d94275bcc4845ef544105a"
          }
        },
        "ef1fca4dadbb4a678c362e0b49406f4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8851e8c3b7f46b19d29faada25a370c",
            "placeholder": "​",
            "style": "IPY_MODEL_b3a98271f84a4885bbd46a0c5574dd73",
            "value": "Unsloth: Tokenizing [&quot;texts&quot;]: 100%"
          }
        },
        "81b450ab70784e1c827e2cbfa825f66a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4274a9a84214a61bbf71dee2c7a869f",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7556358c93844a11a236a07203c1f77c",
            "value": 500
          }
        },
        "7b541fff965346b08fd2617ebbac7a6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1092af317004edcb7d6c4b9fc451a35",
            "placeholder": "​",
            "style": "IPY_MODEL_b0e2cee1e4af4a18ac0e3c9917779a69",
            "value": " 500/500 [00:01&lt;00:00, 482.23 examples/s]"
          }
        },
        "81353d14f3d94275bcc4845ef544105a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8851e8c3b7f46b19d29faada25a370c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3a98271f84a4885bbd46a0c5574dd73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4274a9a84214a61bbf71dee2c7a869f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7556358c93844a11a236a07203c1f77c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1092af317004edcb7d6c4b9fc451a35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0e2cee1e4af4a18ac0e3c9917779a69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}